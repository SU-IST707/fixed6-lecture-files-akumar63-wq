{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9a90dd",
   "metadata": {},
   "source": [
    "# Exercise 1: Implementing a Learning Rate Finder\n",
    "\n",
    "In this exercise, we'll implement a basic learning rate finder that helps identify good learning rates for training neural networks. The learning rate finder works by training the model for a few iterations while exponentially increasing the learning rate and monitoring the loss.\n",
    "\n",
    "## Setup and Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fdbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess Fashion MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train = X_train_full.astype('float32') / 255.0\n",
    "y_train = y_train_full\n",
    "\n",
    "# Create a simple model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34611cfc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 1: Learning Rate Finder Callback\n",
    "\n",
    "First, we'll create a callback that increases the learning rate exponentially and records the loss at each step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c42d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateFinder(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, min_lr=1e-7, max_lr=10, steps=200):\n",
    "        super().__init__()\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.steps = steps\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "        \n",
    "        # Calculate the multiplication factor for each step\n",
    "        # STUDENT TASK: Calculate the factor that, when multiplied steps times,\n",
    "        # goes from min_lr to max_lr\n",
    "        self.lr_factor = # Your code here\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Start with minimum learning rate\n",
    "        #print(f\"Setting min rate {self.min_lr}\")\n",
    "        self.model.optimizer.learning_rate.assign(self.min_lr)\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "        # Get the current learning rate and loss\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        loss = logs['loss']\n",
    "        \n",
    "        # Store the values\n",
    "        self.learning_rates.append(tf.keras.backend.get_value(lr))\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "        # Stop if the loss is not finite (exploding)\n",
    "        if not np.isfinite(loss):\n",
    "            print('Stopping - Loss is not finite!')\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "        \n",
    "        # Increase the learning rate for the next iteration\n",
    "        # STUDENT TASK: Multiply the current learning rate by lr_factor\n",
    "        new_lr = # Your code here\n",
    "        #print(f\"New factor {new_lr}\")\n",
    "        self.model.optimizer.learning_rate.assign(new_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cfda1a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 2: Finding the Learning Rate\n",
    "\n",
    "Now we'll create a function that uses our callback to find a good learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d22e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_learning_rate(model, X, y, batch_size=64, steps=200):\n",
    "    # Initialize the learning rate finder\n",
    "    lr_finder = LearningRateFinder(steps=steps)\n",
    "    \n",
    "    # Compile the model\n",
    "    # STUDENT TASK: Compile the model with SGD optimizer and sparse_categorical_crossentropy\n",
    "    model.compile(\n",
    "        optimizer=# YOUR CODE HERE,\n",
    "        loss=# YOUR CODE HERE,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Calculate the number of samples to use\n",
    "    num_samples = steps * batch_size\n",
    "    \n",
    "    # If we have more samples than we need, take a random subset\n",
    "    if len(X) > num_samples:\n",
    "        idx = np.random.randint(len(X), size=num_samples)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "    \n",
    "    # Train the model with the learning rate finder\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        # STUDENT TASK: Set batch_size and include the lr_finder callback\n",
    "        batch_size=# YOUR CODE HERE,\n",
    "        epochs=1,\n",
    "        callbacks=[# YOUR CODE HERE],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return lr_finder.learning_rates, lr_finder.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c4dcf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 3: Visualizing and Analyzing Results\n",
    "\n",
    "Finally, we'll create a function to plot and analyze the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_rate(learning_rates, losses):\n",
    "    # Remove any infinite or nan losses\n",
    "    valid_idx = np.isfinite(losses)\n",
    "    learning_rates = np.array(learning_rates)[valid_idx]\n",
    "    losses = np.array(losses)[valid_idx]\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(learning_rates, losses)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs. Learning Rate')\n",
    "    \n",
    "    # STUDENT TASK: Find the learning rate with minimum loss\n",
    "    min_loss_idx = # YOUR CODE HERE\n",
    "    best_lr = learning_rates[min_loss_idx]\n",
    "    \n",
    "    # Add a dot at the minimum loss\n",
    "    plt.plot(best_lr, losses[min_loss_idx], 'ro')\n",
    "    \n",
    "    # Add a text annotation\n",
    "    plt.annotate(f'Best LR: {best_lr:.2e}', \n",
    "                xy=(best_lr, losses[min_loss_idx]),\n",
    "                xytext=(best_lr*1.5, losses[min_loss_idx]*1.1),\n",
    "                arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_lr\n",
    "\n",
    "# Now let's run everything!\n",
    "model = create_model()\n",
    "lr_values, loss_values = find_learning_rate(model, X_train, y_train)\n",
    "best_lr = plot_learning_rate(lr_values, loss_values)\n",
    "print(f\"\\nRecommended learning rate: {best_lr:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0396207",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Student Tasks:\n",
    "\n",
    "1. In the `LearningRateFinder` class, calculate the `lr_factor` that will increase the learning rate from `min_lr` to `max_lr` over the specified number of steps.\n",
    "   - Hint: Think about the relationship between exponential growth and multiplication.\n",
    "   - The formula is: `factor = (max_lr/min_lr)^(1/steps)`\n",
    "\n",
    "2. In the `LearningRateFinder` class, implement the learning rate update step.\n",
    "   - You need to multiply the current learning rate by the `lr_factor`\n",
    "\n",
    "3. In the `find_learning_rate` function, complete the model compilation step:\n",
    "   - Choose the appropriate optimizer (SGD)\n",
    "   - Set the loss function for classification\n",
    "   - Remember this is a multi-class problem with integer labels\n",
    "\n",
    "4. In the `find_learning_rate` function, set up the model fitting parameters:\n",
    "   - Set the batch size\n",
    "   - Include the learning rate finder callback\n",
    "\n",
    "5. In the `plot_learning_rate` function, find the index of the minimum loss:\n",
    "   - Use numpy to find the index of the minimum value in the losses array\n",
    "\n",
    "## Extension Questions:\n",
    "\n",
    "1. Why do we use a logarithmic scale for the learning rate?\n",
    "2. What happens if we set the maximum learning rate too high?\n",
    "3. Why might we want to use only a subset of our training data for finding the learning rate?\n",
    "4. How would you modify this code to find a good learning rate for different optimizers (Adam, RMSprop, etc.)?\n",
    "\n",
    "## Expected Output:\n",
    "\n",
    "When run correctly, you should see:\n",
    "- A plot showing the loss vs. learning rate on a log scale\n",
    "- A red dot indicating the point of minimum loss\n",
    "- A annotation showing the recommended learning rate\n",
    "- The recommended learning rate should typically be between 1e-4 and 1e-1\n",
    "\n",
    "Remember: The recommended learning rate is usually about 1/10th of the learning rate at the minimum loss point, as this provides a good balance between learning speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b83ed0",
   "metadata": {},
   "source": [
    "# Exercise 2: Optimizer Comparison with Noisy Data\n",
    "\n",
    "In this exercise, we'll explore how different optimizers perform when training on data with varying levels of noise. We'll learn how adaptive and non-adaptive optimizers respond differently to noisy gradients.\n",
    "\n",
    "## Setup and Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb83f63",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 1: Data Generation\n",
    "\n",
    "First, we'll create a function to generate synthetic data with controllable noise levels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea76295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_data(n_samples=1000, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Generates synthetic classification data with controlled noise\n",
    "    and non-linear decision boundaries\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to generate\n",
    "        noise_level: Controls both cluster overlap and noise magnitude\n",
    "    \"\"\"\n",
    "    # Number of samples per class\n",
    "    n_per_class = n_samples // 4\n",
    "    \n",
    "    # Generate four overlapping clusters in a spiral pattern\n",
    "    t = np.linspace(0, 4*np.pi, n_per_class)\n",
    "    \n",
    "    # First two clusters (class 0)\n",
    "    r1 = 2 + 0.2 * noise_level * np.random.randn(n_per_class)\n",
    "    x1 = r1 * np.cos(t)\n",
    "    y1 = r1 * np.sin(t)\n",
    "    \n",
    "    r2 = 4 + 0.2 * noise_level * np.random.randn(n_per_class)\n",
    "    x2 = r2 * np.cos(t + np.pi/2)\n",
    "    y2 = r2 * np.sin(t + np.pi/2)\n",
    "    \n",
    "    # Second two clusters (class 1)\n",
    "    r3 = 3 + 0.2 * noise_level * np.random.randn(n_per_class)\n",
    "    x3 = r3 * np.cos(t + np.pi/4)\n",
    "    y3 = r3 * np.sin(t + np.pi/4)\n",
    "    \n",
    "    r4 = 5 + 0.2 * noise_level * np.random.randn(n_per_class)\n",
    "    x4 = r4 * np.cos(t + 3*np.pi/4)\n",
    "    y4 = r4 * np.sin(t + 3*np.pi/4)\n",
    "    \n",
    "    # Combine all clusters\n",
    "    X = np.vstack([\n",
    "        np.column_stack([x1, y1]),\n",
    "        np.column_stack([x2, y2]),\n",
    "        np.column_stack([x3, y3]),\n",
    "        np.column_stack([x4, y4])\n",
    "    ])\n",
    "    \n",
    "    # Create labels\n",
    "    y = np.hstack([\n",
    "        np.zeros(2*n_per_class),\n",
    "        np.ones(2*n_per_class)\n",
    "    ])\n",
    "    \n",
    "    # STUDENT TASK\n",
    "    # Add random noise to the data set to make it a little more challenging\n",
    "    X = #YOUR CODE HERE\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    idx = np.random.permutation(len(X))\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95139277",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 2: Model Creation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c175f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Creates a simple neural network classifier\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(16, activation='relu', input_shape=(2,)),\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def get_optimizer(optimizer_name, learning_rate):\n",
    "    \"\"\"\n",
    "    Creates an optimizer instance based on name and learning rate\n",
    "    \n",
    "    Args:\n",
    "        optimizer_name: String name of optimizer ('sgd', 'adam', etc.)\n",
    "        learning_rate: Learning rate to use\n",
    "    \"\"\"\n",
    "    # STUDENT TASK: Create and return the appropriate optimizer\n",
    "    # Include momentum=0.9 for SGD\n",
    "    if optimizer_name.lower() == 'sgd':\n",
    "        return # YOUR CODE HERE\n",
    "    elif optimizer_name.lower() == 'adam':\n",
    "        return # YOUR CODE HERE\n",
    "    elif optimizer_name.lower() == 'rmsprop':\n",
    "        return # YOUR CODE HERE\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac0bf9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 3: Training and Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f4fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X, y, optimizer_name, learning_rate, noise_level, \n",
    "                      epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains a model and returns training history\n",
    "    \"\"\"\n",
    "    # Split data into train/test sets\n",
    "    # STUDENT TASK: Split the data 80/20 using train_test_split\n",
    "    X_train, X_test, y_train, y_test = # YOUR CODE HERE\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = create_model()\n",
    "    optimizer = get_optimizer(optimizer_name, learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(X_train, y_train,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       validation_data=(X_test, y_test),\n",
    "                       verbose=0)\n",
    "    \n",
    "    # Get final test accuracy\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    return history.history, test_acc\n",
    "\n",
    "def compare_optimizers(noise_levels=[0.1, 0.5, 1.0], \n",
    "                      learning_rates=[0.001, 0.01, 0.1]):\n",
    "    \"\"\"\n",
    "    Compares optimizers across different noise levels and learning rates\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    optimizers = ['SGD', 'Adam', 'RMSprop']\n",
    "    \n",
    "    # STUDENT TASK: Create nested loops to test all combinations\n",
    "    # Loop through noise levels, optimizers, and learning rates\n",
    "    for # YOUR CODE HERE:\n",
    "        # Generate noisy data\n",
    "        X, y = generate_noisy_data(noise_level=noise_level)\n",
    "        \n",
    "        # Train with current configuration\n",
    "        history, test_acc = train_and_evaluate(X, y, optimizer, lr, noise_level)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Noise Level': noise_level,\n",
    "            'Optimizer': optimizer,\n",
    "            'Learning Rate': lr,\n",
    "            'Test Accuracy': test_acc\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c24896",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 4: Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2114ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_df):\n",
    "    \"\"\"\n",
    "    Creates visualizations of the results\n",
    "    \"\"\"\n",
    "    # Create a heatmap for each optimizer\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, optimizer in enumerate(['SGD', 'Adam', 'RMSprop']):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        \n",
    "        # STUDENT TASK: Pivot the data to create a heatmap\n",
    "        # Rows should be noise levels, columns should be learning rates\n",
    "        pivot_data = # YOUR CODE HERE\n",
    "        \n",
    "        sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd')\n",
    "        plt.title(f'{optimizer} Performance')\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Noise Level')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the experiment\n",
    "results = compare_optimizers()\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f626ccba",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Student Tasks:\n",
    "\n",
    "1. In `generate_noisy_data`, add Gaussian noise to the features:\n",
    "   - Use np.random.normal with the given noise_level as the scale\n",
    "   - The noise should be added to the original features X\n",
    "\n",
    "2. In `get_optimizer`, implement the creation of each optimizer type:\n",
    "   - SGD with momentum=0.9\n",
    "   - Adam with default parameters\n",
    "   - RMSprop with default parameters\n",
    "\n",
    "3. In `train_and_evaluate`, split the data into training and test sets:\n",
    "   - Use sklearn's train_test_split\n",
    "   - Use an 80/20 split ratio\n",
    "   - Set random_state=42 for reproducibility\n",
    "\n",
    "4. In `compare_optimizers`, implement the nested loops:\n",
    "   - Outer loop over noise levels\n",
    "   - Middle loop over optimizers\n",
    "   - Inner loop over learning rates\n",
    "\n",
    "5. In `plot_results`, create the pivot table for the heatmap:\n",
    "   - Filter for the current optimizer\n",
    "   - Pivot the data with noise levels as index and learning rates as columns\n",
    "   - Values should be test accuracy\n",
    "\n",
    "## Extension Questions:\n",
    "\n",
    "1. Why do adaptive optimizers (Adam, RMSprop) typically perform better with noisy data?\n",
    "2. How does the optimal learning rate change with noise level for each optimizer?\n",
    "3. What happens if we increase the number of training epochs? Does it affect different optimizers differently?\n",
    "4. How would you modify this experiment to test the optimizers' robustness to different types of noise (e.g., outliers vs. Gaussian noise)?\n",
    "\n",
    "## Expected Output:\n",
    "\n",
    "When run correctly, you should see:\n",
    "- Three heatmaps showing the performance of each optimizer\n",
    "- Performance should generally decrease with increasing noise\n",
    "- Adaptive optimizers should show more robustness to noise\n",
    "- Higher learning rates should show more sensitivity to noise\n",
    "\n",
    "The visualization should help identify which optimizer is most robust across different noise levels and learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9efd3f",
   "metadata": {},
   "source": [
    "# Exercise 3: Custom Learning Rate Schedule\n",
    "\n",
    "In this exercise, we'll create a custom learning rate schedule that combines multiple scheduling strategies. Our schedule will implement a \"warm-up\" period, followed by exponential decay, and include periodic \"restarts\" where the learning rate temporarily increases.\n",
    "\n",
    "## Setup and Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad5054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "# Take 5000 samples\n",
    "n_samples = 5000\n",
    "indices = np.random.permutation(len(X_train_full))[:n_samples]\n",
    "X_train = X_train_full[indices] / 255.0\n",
    "y_train = y_train_full[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd8f99",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 1: Custom Learning Rate Schedule Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmupCosineRestart(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Custom learning rate schedule with:\n",
    "    1. Linear warmup period\n",
    "    2. Cosine decay\n",
    "    3. Periodic restarts\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_learning_rate, warmup_steps, decay_steps, alpha=0.0):\n",
    "        super(WarmupCosineRestart, self).__init__()\n",
    "        \n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "        self.alpha = alpha  # Minimum learning rate factor\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        # Convert step to float32\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        \n",
    "        # STUDENT TASK 1: Implement warmup phase\n",
    "        # During warmup, LR should increase linearly from 0 to initial_learning_rate\n",
    "        warmup_lr = # YOUR CODE HERE\n",
    "        \n",
    "        # STUDENT TASK 2: Implement cosine decay with restart\n",
    "        # Calculate the current cycle and progress within cycle\n",
    "        cycle = tf.floor(step / self.decay_steps)\n",
    "        current_step = step - (cycle * self.decay_steps)\n",
    "        \n",
    "        # Cosine decay formula\n",
    "        cosine_decay = # YOUR CODE HERE\n",
    "        \n",
    "        # Combine warmup and decay\n",
    "        lr = tf.cond(\n",
    "            step < self.warmup_steps,\n",
    "            lambda: warmup_lr,\n",
    "            lambda: self.initial_learning_rate * cosine_decay\n",
    "        )\n",
    "        \n",
    "        return lr\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_learning_rate\": self.initial_learning_rate,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"decay_steps\": self.decay_steps,\n",
    "            \"alpha\": self.alpha\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c97389",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 2: Training Loop and Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab50511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Creates a simple CNN model\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def plot_schedule(schedule, steps):\n",
    "    \"\"\"Plots the learning rate schedule\"\"\"\n",
    "    lrs = [schedule(step).numpy() for step in range(steps)]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(lrs)\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def train_and_compare_schedules(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Trains models with different learning rate schedules and compares them\"\"\"\n",
    "    \n",
    "    # Calculate steps per epoch\n",
    "    batch_size = 32\n",
    "    steps_per_epoch = len(X_train) // batch_size\n",
    "    \n",
    "    # STUDENT TASK 3: Create three different schedule configurations\n",
    "    schedules = {\n",
    "        'standard': tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "            # YOUR CODE HERE\n",
    "        ),\n",
    "        'warmup_cosine': WarmupCosineRestart(\n",
    "            # YOUR CODE HERE\n",
    "        ),\n",
    "        'custom': # Create your own schedule configuration\n",
    "    }\n",
    "    \n",
    "    histories = {}\n",
    "    \n",
    "    # Train with each schedule\n",
    "    for name, schedule in schedules.items():\n",
    "        # STUDENT TASK 4: Create and compile model\n",
    "        model = create_model()\n",
    "        model.compile(\n",
    "            # YOUR CODE HERE\n",
    "        )\n",
    "        \n",
    "        # Reshape data for CNN\n",
    "        X_train_reshaped = X_train.reshape(-1, 28, 28, 1)\n",
    "        X_test_reshaped = X_test.reshape(-1, 28, 28, 1)\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_reshaped, y_train,\n",
    "            epochs=10,\n",
    "            validation_data=(X_test_reshaped, y_test),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        histories[name] = history.history\n",
    "    \n",
    "    return histories\n",
    "\n",
    "def visualize_results(histories):\n",
    "    \"\"\"Plots training curves for different schedules\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # STUDENT TASK 5: Create subplots for loss and accuracy\n",
    "    # Plot training curves for each schedule\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, history in histories.items():\n",
    "        # YOUR CODE HERE - Plot loss\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, history in histories.items():\n",
    "        # YOUR CODE HERE - Plot accuracy\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e7098",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 3: Running the Experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fda201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and visualize the custom schedule\n",
    "schedule = WarmupCosineRestart(\n",
    "    initial_learning_rate=0.001,\n",
    "    warmup_steps=1000,\n",
    "    decay_steps=4000,\n",
    "    alpha=0.1\n",
    ")\n",
    "\n",
    "# Plot the schedule\n",
    "plot_schedule(schedule, 15000)\n",
    "\n",
    "# Train models and compare results\n",
    "histories = train_and_compare_schedules(X_train, y_train, X_test, y_test)\n",
    "visualize_results(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47320d88",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Student Tasks:\n",
    "\n",
    "1. In the `WarmupCosineRestart` class, implement the warmup phase:\n",
    "   - Learning rate should increase linearly from 0 to initial_learning_rate\n",
    "   - Use tf.minimum to ensure we don't exceed warmup_steps\n",
    "\n",
    "2. In the `WarmupCosineRestart` class, implement the cosine decay:\n",
    "   - Use tf.cos and tf.cast to create the decay\n",
    "   - Formula: 0.5 * (1 + cos(Ï€ * x)) where x goes from 0 to 1 in each cycle\n",
    "\n",
    "3. In `train_and_compare_schedules`, create three learning rate schedules:\n",
    "   - Standard exponential decay\n",
    "   - Warmup cosine with restart\n",
    "   - Your own custom configuration of the WarmupCosineRestart\n",
    "\n",
    "4. Complete the model compilation in `train_and_compare_schedules`:\n",
    "   - Use the appropriate optimizer with the schedule\n",
    "   - Set loss and metrics\n",
    "\n",
    "5. In `visualize_results`, implement the plotting code:\n",
    "   - Create loss subplot with all schedules\n",
    "   - Create accuracy subplot with all schedules\n",
    "   - Add appropriate labels and legend\n",
    "\n",
    "## Extension Questions:\n",
    "\n",
    "1. How does the warmup period affect the early stages of training?\n",
    "2. What are the advantages and disadvantages of learning rate restarts?\n",
    "3. How would you modify the schedule for a very deep network?\n",
    "4. What considerations would you make when choosing the warmup_steps and decay_steps parameters?\n",
    "\n",
    "## Expected Output:\n",
    "\n",
    "When run correctly, you should see:\n",
    "- A plot of the learning rate schedule showing warmup, decay, and restarts\n",
    "- Training curves comparing different schedules showing:\n",
    "  - Loss generally decreasing faster with the custom schedule\n",
    "  - Potential spikes in loss during learning rate restarts\n",
    "  - Better final performance with the custom schedule\n",
    "\n",
    "The plot should clearly show the warmup period, followed by cosine decay cycles with restarts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f0f11",
   "metadata": {},
   "source": [
    "# Exercise 4: Momentum and Learning Rate Interaction Study\n",
    "\n",
    "In this exercise, we'll explore how momentum and learning rate interact during training. We'll create a systematic study of different combinations and visualize their effects on model training.\n",
    "\n",
    "## Setup and Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import pandas as pd\n",
    "\n",
    "# Load and preprocess Fashion MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Take a subset of data for faster experimentation\n",
    "n_samples = 10000\n",
    "X_train = X_train[:n_samples]\n",
    "y_train = y_train[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfd3fd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 1: Training Infrastructure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seed=42):\n",
    "    \"\"\"Creates a simple neural network\"\"\"\n",
    "    tf.random.set_seed(seed)\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "class TrainingMonitor(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Monitors training metrics including gradient norms\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.batch_losses = []\n",
    "        self.loss_changes = []  # Track relative loss changes\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        # STUDENT TASK 1: Store the batch loss\n",
    "        # Hint: Use logs dictionary\n",
    "        self.batch_losses.append(# YOUR CODE HERE)\n",
    "        \n",
    "        \n",
    "        # STUDENT TASK 2: Calculate relative loss change \n",
    "        # Calculate the absolute change in loss relative to the loss from the previous batch\n",
    "        if len(self.batch_losses) > 1:\n",
    "            loss_change = #YOUR CODE HERE\n",
    "            self.loss_changes.append(loss_change)\n",
    "        else:\n",
    "            self.loss_changes.append(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04922a6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 2: Training Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_params(learning_rate, momentum, use_nesterov=False):\n",
    "    \"\"\"Trains model with specific learning rate and momentum settings\"\"\"\n",
    "    model = create_model()\n",
    "    \n",
    "    # STUDENT TASK 3: Create SGD optimizer with given parameters\n",
    "    optimizer = # YOUR CODE HERE\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    monitor = TrainingMonitor()\n",
    "    \n",
    "    # Train for a small number of epochs\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=5,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[monitor],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'history': history.history,\n",
    "        'batch_losses': monitor.batch_losses,\n",
    "        'loss_changes': monitor.loss_changes,\n",
    "        'final_loss': history.history['loss'][-1],\n",
    "        'final_accuracy': history.history['accuracy'][-1]\n",
    "    }\n",
    "\n",
    "def run_parameter_study(learning_rates, momentums):\n",
    "    \"\"\"Runs training with different combinations of learning rates and momentums\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # STUDENT TASK 4: Create nested loops to test all combinations\n",
    "    # Include both standard momentum and Nesterov momentum\n",
    "    for # YOUR CODE HERE:\n",
    "        \n",
    "        # Store results in a list of dictionaries\n",
    "        results.append({\n",
    "            'learning_rate': lr,\n",
    "            'momentum': momentum,\n",
    "            'nesterov': use_nesterov,\n",
    "            'final_loss': metrics['final_loss'],\n",
    "            'final_accuracy': metrics['final_accuracy'],\n",
    "            'avg_gradient_norm': np.mean(metrics['gradient_norms'])\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311c961",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 3: Visualization Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmaps(results_df):\n",
    "    \"\"\"Creates heatmaps for loss and accuracy across parameter combinations\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    \n",
    "    # STUDENT TASK 5: Create four heatmaps\n",
    "    # Standard momentum accuracy\n",
    "    std_acc_data = # YOUR CODE HERE - Pivot table for standard momentum accuracy\n",
    "    sns.heatmap(std_acc_data, ax=axes[0, 0], cmap='viridis', annot=True)\n",
    "    axes[0, 0].set_title('Standard Momentum - Accuracy')\n",
    "    \n",
    "    # Nesterov momentum accuracy\n",
    "    nesterov_acc_data = # YOUR CODE HERE - Pivot table for Nesterov momentum accuracy\n",
    "    sns.heatmap(nesterov_acc_data, ax=axes[0, 1], cmap='viridis', annot=True)\n",
    "    axes[0, 1].set_title('Nesterov Momentum - Accuracy')\n",
    "    \n",
    "    # Standard momentum gradient norms\n",
    "    std_stablity_data = # YOUR CODE HERE - Pivot table for standard momentum stability (using loss changes)\n",
    "    sns.heatmap(std_stablity_data, ax=axes[1, 0], cmap='rocket', annot=True)\n",
    "    axes[1, 0].set_title('Standard Momentum - Loss stability')\n",
    "    \n",
    "    # Nesterov momentum gradient norms\n",
    "    nesterov_stability_data = # YOUR CODE HERE - Pivot table for Nesterov momentum stability\n",
    "    sns.heatmap(nesterov_stability_data, ax=axes[1, 1], cmap='rocket', annot=True)\n",
    "    axes[1, 1].set_title('Nesterov Momentum - Loss stability')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5d483",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 4: Running the Experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ec1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter ranges\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "momentums = [0.0, 0.5, 0.9, 0.99]\n",
    "\n",
    "# Run the study\n",
    "results = run_parameter_study(learning_rates, momentums)\n",
    "\n",
    "# Create visualizations\n",
    "fig = create_heatmaps(results)\n",
    "plt.show()\n",
    "\n",
    "# Print best configurations\n",
    "print(\"\\nBest configurations:\")\n",
    "for use_nesterov in [False, True]:\n",
    "    subset = results[results['nesterov'] == use_nesterov]\n",
    "    best_idx = subset['final_accuracy'].idxmax()\n",
    "    best_config = subset.loc[best_idx]\n",
    "    print(f\"\\n{'Nesterov' if use_nesterov else 'Standard'} Momentum:\")\n",
    "    print(f\"Learning Rate: {best_config['learning_rate']}\")\n",
    "    print(f\"Momentum: {best_config['momentum']}\")\n",
    "    print(f\"Accuracy: {best_config['final_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2cae2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Student Tasks:\n",
    "\n",
    "1. In the `TrainingMonitor` class, implement batch loss storage:\n",
    "   - Extract the 'loss' value from the logs dictionary\n",
    "   - Append it to batch_losses list\n",
    "\n",
    "2. In the `TrainingMonitor` class, calculate gradient norm:\n",
    "   - Use tf.linalg.global_norm to compute the norm of gradients\n",
    "   - Return the norm as a scalar value\n",
    "\n",
    "3. In `train_model_with_params`, create the SGD optimizer:\n",
    "   - Use tf.keras.optimizers.SGD\n",
    "   - Include learning_rate, momentum, and nesterov parameters\n",
    "\n",
    "4. In `run_parameter_study`, implement the nested loops:\n",
    "   - Loop over learning rates, momentums, and nesterov options\n",
    "   - Call train_model_with_params with each combination\n",
    "   - Store results in the results list\n",
    "\n",
    "5. In `create_heatmaps`, create the pivot tables:\n",
    "   - Filter data for standard/Nesterov momentum\n",
    "   - Create pivot tables with learning rates as columns and momentum as index\n",
    "   - Use 'final_accuracy' and 'avg_loss_change' as values\n",
    "\n",
    "## Extension Questions:\n",
    "\n",
    "1. Why do some combinations of learning rate and momentum lead to unstable training?\n",
    "2. How does the interaction between learning rate and momentum change with deeper networks?\n",
    "3. What role does the loss change play in understanding training dynamics?\n",
    "4. Why might Nesterov momentum perform better than standard momentum in some cases?\n",
    "\n",
    "## Expected Output:\n",
    "\n",
    "When run correctly, you should see:\n",
    "- Four heatmaps showing the interaction between learning rate and momentum\n",
    "- Clear patterns showing optimal combinations for each momentum type\n",
    "- Potential instability with high learning rates and high momentum\n",
    "- Different patterns for standard vs Nesterov momentum\n",
    "- Summary of best configurations for each momentum type\n",
    "\n",
    "The visualization should help identify safe and risky parameter combinations, and show how Nesterov momentum might provide more stability in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadeb86",
   "metadata": {},
   "source": [
    "# Exercise 5: Adaptive Learning Rate Emergency\n",
    "\n",
    "In this exercise, you'll implement a custom callback that monitors training stability and automatically adjusts the learning rate when problems are detected. This represents a real-world scenario where you need to rescue training that's becoming unstable.\n",
    "\n",
    "## Setup and Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ad4a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load and preprocess Fashion MNIST\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3029c4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 1: Training Monitor Callback\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256628d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingEmergencyCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Monitors training stability and adjusts learning rate when needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 patience=5,\n",
    "                 loss_spike_threshold=1.5,\n",
    "                 min_lr=1e-6):\n",
    "        super().__init__()\n",
    "        self.patience = patience\n",
    "        self.grad_norm_threshold = grad_norm_threshold\n",
    "        self.loss_spike_threshold = loss_spike_threshold\n",
    "        self.min_lr = min_lr\n",
    "        \n",
    "        self.loss_history = []\n",
    "        self.grad_history = []\n",
    "        self.lr_history = []\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        # STUDENT TASK 1: Initialize monitoring variables\n",
    "        # Keep track of consecutive problems and best loss\n",
    "        self.consecutive_problems = # YOUR CODE HERE\n",
    "        self.best_loss = # YOUR CODE HERE\n",
    "        \n",
    "    def compute_gradient_norm(self):\n",
    "        \"\"\"Computes the global norm of the model's gradients\"\"\"\n",
    "        # Get gradients\n",
    "        gradients = self.model.optimizer.get_gradients(\n",
    "            self.model.total_loss,\n",
    "            self.model.trainable_weights\n",
    "        )\n",
    "        \n",
    "        # STUDENT TASK 2: Calculate and return gradient norm\n",
    "        # Use tf.linalg.global_norm\n",
    "        return # YOUR CODE HERE\n",
    "    \n",
    "    def check_training_problems(self, logs):\n",
    "        \"\"\"Checks for potential training problems\"\"\"\n",
    "        current_loss = logs['loss']\n",
    "        current_lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n",
    "        grad_norm = self.compute_gradient_norm()\n",
    "        \n",
    "        # Store history\n",
    "        self.loss_history.append(current_loss)\n",
    "        self.grad_history.append(grad_norm)\n",
    "        self.lr_history.append(current_lr)\n",
    "        \n",
    "        problems = []\n",
    "        \n",
    "        # STUDENT TASK 3: Implement problem detection\n",
    "        # Check for:\n",
    "        # 1. Loss spike (current loss much higher than best loss)\n",
    "        # 2. NaN/Inf values in loss\n",
    "        # 3. Consistently increasing loss\n",
    "        if # YOUR CODE HERE:  # Loss spike check\n",
    "            problems.append(\"Loss spike detected\")\n",
    "            \n",
    "        if # YOUR CODE HERE:  # Gradient explosion check\n",
    "            problems.append(\"Gradient explosion detected\")\n",
    "            \n",
    "        if # YOUR CODE HERE:  # NaN/Inf check\n",
    "            problems.append(\"Consistently increasing loss\")\n",
    "            \n",
    "        return problems\n",
    "    \n",
    "    def adjust_learning_rate(self, problems):\n",
    "        \"\"\"Adjusts learning rate based on detected problems\"\"\"\n",
    "        current_lr = tf.keras.backend.get_value(self.model.optimizer.learning_rate)\n",
    "        \n",
    "        # STUDENT TASK 4: Implement learning rate adjustment\n",
    "        # Reduce learning rate if there are problems\n",
    "        # Make sure new lr isn't below min_lr\n",
    "        if problems:\n",
    "            new_lr = # YOUR CODE HERE\n",
    "            new_lr = max(new_lr, self.min_lr)\n",
    "            self.model.optimizer.learning_rate.assign(new_lr)\n",
    "            logger.info(f\"Learning rate adjusted from {current_lr} to {new_lr}\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        problems = self.check_training_problems(logs)\n",
    "        \n",
    "        if problems:\n",
    "            self.consecutive_problems += 1\n",
    "            logger.warning(f\"Training problems detected: {problems}\")\n",
    "            \n",
    "            if self.consecutive_problems >= self.patience:\n",
    "                self.adjust_learning_rate(problems)\n",
    "                self.consecutive_problems = 0\n",
    "        else:\n",
    "            self.consecutive_problems = 0\n",
    "            current_loss = logs['loss']\n",
    "            if current_loss < self.best_loss:\n",
    "                self.best_loss = current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2c40bc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 2: Training Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Creates a model prone to training instability\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "def train_with_emergency_monitoring(initial_lr=0.1, epochs=10, patience=3):\n",
    "    \"\"\"Trains model with emergency monitoring\"\"\"\n",
    "    model = create_model()\n",
    "    \n",
    "    # STUDENT TASK 5: Create optimizer and compile model\n",
    "    optimizer = # YOUR CODE HERE\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Create callback\n",
    "    emergency_cb = TrainingEmergencyCallback(\n",
    "        patience=patience,\n",
    "        grad_norm_threshold=10.0,\n",
    "        loss_spike_threshold=1.5\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[emergency_cb],\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history, emergency_cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c4ef4c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 3: Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f7a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(callback):\n",
    "    \"\"\"Visualizes training metrics and learning rate adjustments\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "    \n",
    "    # Plot loss history\n",
    "    ax1.plot(callback.loss_history)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Plot learning rate changes\n",
    "    ax2.plot(callback.lr_history)\n",
    "    ax2.set_title('Learning Rate')\n",
    "    ax2.set_ylabel('Learning Rate')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Set the logging level if you want to adjust the verbosity \n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Run training\n",
    "history, callback = train_with_emergency_monitoring(patience=5)\n",
    "plot_training_metrics(callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448634e2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Student Tasks:\n",
    "\n",
    "1. In `TrainingEmergencyCallback.__init__`, initialize monitoring variables:\n",
    "   - Set consecutive_problems to 0\n",
    "   - Set best_loss to float('inf')\n",
    "\n",
    "2. In `compute_gradient_norm`, calculate the gradient norm:\n",
    "   - Use tf.linalg.global_norm to compute norm of gradients\n",
    "   - Convert result to a numpy scalar\n",
    "\n",
    "3. In `check_training_problems`, implement problem detection:\n",
    "   - Check if current loss exceeds best_loss * loss_spike_threshold\n",
    "   - Check if gradient norm exceeds grad_norm_threshold\n",
    "   - Check for NaN/Inf values in loss\n",
    "\n",
    "4. In `adjust_learning_rate`, implement learning rate adjustment:\n",
    "   - Reduce current learning rate by factor of 2\n",
    "   - Ensure new rate doesn't fall below min_lr\n",
    "   - Return True if adjustment was made\n",
    "\n",
    "5. In `train_with_emergency_monitoring`, create the optimizer:\n",
    "   - Use SGD with the specified initial learning rate\n",
    "   - Add momentum=0.9 to make training more stable\n",
    "\n",
    "## Extension Questions:\n",
    "\n",
    "1. What happens when you change the patience values?  Try several.\n",
    "2. How would you modify the callback to also adjust batch size when problems are detected?\n",
    "2. What other metrics might be useful to monitor for training stability?\n",
    "3. How would you implement a \"recovery mode\" that tries to restore weights to the last stable state?\n",
    "4. How would you modify the callback to work with adaptive optimizers like Adam?\n",
    "\n",
    "## Expected Output:\n",
    "\n",
    "When run correctly, you should see:\n",
    "- Training progress with occasional warnings about detected problems\n",
    "- Learning rate adjustments when problems persist\n",
    "- Two plots showing:\n",
    "  - Training loss over time\n",
    "  - Learning rate changes on log scale\n",
    "- Successful training completion with stable final epochs\n",
    "\n",
    "The visualization should show clear correlations between detected problems and learning rate adjustments."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
